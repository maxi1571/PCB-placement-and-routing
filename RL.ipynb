{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "import main\n",
    "from gym.spaces import Discrete, Box\n",
    "import random\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "class PsecWorld(Env):\n",
    "    def __init__(self, state, W, H):\n",
    "        self.H= H\n",
    "        self.W = W\n",
    "        self.action_space = Discrete(len(W))\n",
    "        self.observation_space= np.array(range(1,len(W)+2))\n",
    "        self.state = state\n",
    "        self.stop = 100\n",
    "        \n",
    "    def reset(self,state):\n",
    "        self.state = state\n",
    "        self.stop = 100\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        action -= 1\n",
    "        self.action = action\n",
    "        self.stop -= 1\n",
    "        r, s_ = main.move(self.state,self.W, self.H, action)\n",
    "    \n",
    "        self.state = s_\n",
    "        if r == 0:\n",
    "            reward = 0\n",
    "        if r > 0:\n",
    "            reward = 1\n",
    "        if r< 0:\n",
    "            reward = -1\n",
    "        if self.stop <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseq=[[1,2,4,6,3,5],[3,2,6,1,4,5]]\n",
    "w=[1,2,3,2,1,1]\n",
    "h=[2,1,4,3,3,2]\n",
    "action=3\n",
    "state = pseq[0]\n",
    "state.append(action)\n",
    "\n",
    "env = PsecWorld(state, w, h)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-d1f61bf711a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "env.observation_space.shape\n",
    "y = Box(low=np.array([[0, 0]]), high=np.array([[4,4]]))\n",
    "y.shape\n",
    "X= np.array(range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[[1]\n",
      " [2]\n",
      " [4]\n",
      " [6]\n",
      " [3]\n",
      " [5]\n",
      " [1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(env.action_space.n)\n",
    "print(state)\n",
    "state = np.reshape(state,[-1,env.observation_space.shape[0]])\n",
    "state\n",
    "env.observation_space.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 2, 1, 4, 6, 5, 4], 0, False, {})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 6, 3, 5, 3]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "class Memory:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.batch_s = []\n",
    "        self.batch_a = []\n",
    "        self.batch_r = []\n",
    "        self.batch_gae_r = [] #this gets set in agent make_gae which is called once on first training on memory\n",
    "        self.batch_s_ = []\n",
    "        self.batch_done = []\n",
    "        self.GAE_CALCULATED_Q = False #make sure make_gae can only be called once\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size):\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            s,a,r,gae_r,s_,d = [],[],[],[],[],[]\n",
    "            pos = np.random.randint(len(self.batch_s)) #random position\n",
    "            s.append(self.batch_s[pos])\n",
    "            a.append(self.batch_a[pos])\n",
    "            r.append(self.batch_r[pos])\n",
    "            gae_r.append(self.batch_gae_r[pos])\n",
    "            s_.append(self.batch_s_[pos])\n",
    "            d.append(self.batch_done[pos])\n",
    "        return s,a,r,gae_r,s_,d #return randomized batches\n",
    "\n",
    "\n",
    "    def store(self, s, a, s_, r, done):\n",
    "\n",
    "        self.batch_s.append(s)\n",
    "        self.batch_a.append(a)\n",
    "        self.batch_r.append(r)\n",
    "        self.batch_s_.append(s_)\n",
    "        self.batch_done.append(done)\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "\n",
    "        self.batch_s.clear()\n",
    "        self.batch_a.clear()\n",
    "        self.batch_r.clear()\n",
    "        self.batch_s_.clear()\n",
    "        self.batch_done.clear()\n",
    "        self.GAE_CALCULATED_Q = False\n",
    "\n",
    "\n",
    "    def cnt_samples(self):\n",
    "        return len(self.batch_s)\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,action_n, state_dim, training_batch_size):\n",
    "        \n",
    "        self.action_n = action_n\n",
    "        self.state_dim = state_dim        \n",
    "        #CONSTANTS\n",
    "        self.TRAINING_BATCH_SIZE = training_batch_size\n",
    "        self.TARGET_UPDATE_ALPHA = 0.95\n",
    "        self.GAMMA = 0.99\n",
    "        self.GAE_LAMBDA = 0.95\n",
    "        self.CLIPPING_LOSS_RATIO = 0.1\n",
    "        self.ENTROPY_LOSS_RATIO = 0.001\n",
    "        self.TARGET_UPDATE_ALPHA = 0.9\n",
    "        #create actor and critic neural networks\n",
    "        self.critic_network = self._build_critic_network()\n",
    "        self.actor_network = self._build_actor_network()\n",
    "        #for the loss function, additionally \"old\" predicitons are required from before the last update.\n",
    "        #therefore create another networtk. Set weights to be identical for now.\n",
    "        self.actor_old_network = self._build_actor_network()\n",
    "        self.actor_old_network.set_weights(self.actor_network.get_weights()) \n",
    "        #for getting an action (predict), the model requires it's ususal input, but advantage and old_prediction is only used for loss(training). So create dummys for prediction only\n",
    "        self.dummy_advantage = np.zeros((1, 1))\n",
    "        self.dummy_old_prediciton = np.zeros((1, self.action_n))\n",
    "        #our transition memory buffer        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "\n",
    "    \n",
    "    def _build_actor_network(self):\n",
    "\n",
    "        #define inputs. Advantage and old_prediction are required to pass to the ppo_loss funktion\n",
    "        state = K.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        advantage = K.layers.Input(shape=(1,),name='advantage_input')\n",
    "        old_prediction = K.layers.Input(shape=(self.action_n,),name='old_prediction_input')\n",
    "        #define hidden layers\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense1')(state)\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense2')(dense)\n",
    "        #connect layers, output action using softmax activation\n",
    "        policy = K.layers.Dense(self.action_n, activation=\"softmax\", name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        actor_network = K.Model(inputs = [state,advantage,old_prediction], outputs = policy)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        actor_network.compile(\n",
    "            optimizer='Adam',\n",
    "            loss = self.ppo_loss(advantage=advantage,old_prediction=old_prediction)\n",
    "            )\n",
    "        #summary and return       \n",
    "        actor_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return actor_network\n",
    "\n",
    "\n",
    "    def _build_critic_network(self):\n",
    "\n",
    "        #define input layer\n",
    "        state = K.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        #define hidden layers\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense1')(state)\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense2')(dense)\n",
    "        #connect the layers to a 1-dim output: scalar value of the state (= Q value or V(s))\n",
    "        V = K.layers.Dense(1, name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        critic_network = K.Model(inputs=state, outputs=V)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        critic_network.compile(optimizer='Adam',loss = 'mean_squared_error')\n",
    "        #summary and return           \n",
    "        critic_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return critic_network\n",
    "    \n",
    "\n",
    "    def ppo_loss(self, advantage, old_prediction):\n",
    "\n",
    "        #refer to Keras custom loss function intro to understand why we define a funciton inside a function.\n",
    "        def loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred #y_true is taken action one_hot(in deterministic case) and pred is a softmax vector. prob is the probability of the taken aciton.\n",
    "            old_prob = y_true * old_prediction\n",
    "            ratio = prob / (old_prob + 1e-10)\n",
    "            clip_ratio = K.backend.clip(ratio, min_value=1 - self.CLIPPING_LOSS_RATIO, max_value=1 + self.CLIPPING_LOSS_RATIO)\n",
    "            surrogate1 = ratio * advantage\n",
    "            surrogate2 = clip_ratio * advantage\n",
    "            entropy_loss = (prob * K.backend.log(prob + 1e-10)) #optionally add the entropy loss to avoid getting stuck on local minima\n",
    "            ppo_loss = -K.backend.mean(K.backend.minimum(surrogate1,surrogate2) + self.ENTROPY_LOSS_RATIO * entropy_loss)\n",
    "            return ppo_loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def make_gae(self):\n",
    "\n",
    "        gae = 0\n",
    "        mask = 0\n",
    "        for i in reversed(range(self.memory.cnt_samples)):\n",
    "            mask = 0 if self.memory.batch_done[i] else 1\n",
    "            v = self.get_v(self.memory.batch_s[i])\n",
    "            delta = self.memory.batch_r[i] + self.GAMMA * self.get_v(self.memory.batch_s_[i]) * mask - v\n",
    "            gae = delta + self.GAMMA *  self.GAE_LAMBDA * mask * gae\n",
    "            self.memory.batch_gae_r.append(gae+v)\n",
    "        self.memory.batch_gae_r.reverse()\n",
    "        self.memory.GAE_CALCULATED_Q = True\n",
    "\n",
    "\n",
    "    def update_tartget_network(self):\n",
    "\n",
    "        alpha = self.TARGET_UPDATE_ALPHA\n",
    "        actor_weights = np.array(self.actor_network.get_weights())\n",
    "        actor_tartget_weights = np.array(self.actor_old_network.get_weights())\n",
    "        new_weights = alpha*actor_weights + (1-alpha)*actor_tartget_weights\n",
    "        self.actor_old_network.set_weights(new_weights)\n",
    "\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "\n",
    "        #assert isinstance(state,np.ndarray)\n",
    "        #reshape for predict_on_batch which requires 2d-arrays\n",
    "        state = np.reshape(state,[-1,self.state_dim[0]])\n",
    "        #the probability list for each action is the output of the actor network given a state\n",
    "        prob = self.actor_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton]).flatten()\n",
    "        #action is chosen by random with the weightings accoring to the probability\n",
    "        action = np.random.choice(self.action_n,p=prob)\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_network(self):\n",
    "\n",
    "        #important: make gae type rewards BEFORE getting random batches if not done yet\n",
    "        if not self.memory.GAE_CALCULATED_Q:\n",
    "            self.make_gae()\n",
    "        #get randomized mini batches\n",
    "        states,actions,rewards,gae_r,next_states,dones = self.memory.get_batch(self.TRAINING_BATCH_SIZE)\n",
    "       \n",
    "        #create np array batches for training\n",
    "        batch_s = np.vstack(states)\n",
    "        batch_a = np.vstack(actions)\n",
    "        batch_gae_r = np.vstack(gae_r)\n",
    "        #get values of states in batch\n",
    "        batch_v = self.get_v(batch_s)\n",
    "        #calc advantages. required for actor loss. \n",
    "        batch_advantage = batch_gae_r - batch_v\n",
    "        batch_advantage = K.utils.normalize(batch_advantage) #\n",
    "        #calc old_prediction. Required for actor loss.\n",
    "        batch_old_prediction = self.get_old_prediction(batch_s)\n",
    "        #one-hot the actions. Actions will be the target for actor.\n",
    "        batch_a_final = np.zeros(shape=(len(batch_a), self.action_n))\n",
    "        batch_a_final[:, batch_a.flatten()] = 1\n",
    "\n",
    "        #commit training\n",
    "        self.actor_network.fit(x=[batch_s, batch_advantage, batch_old_prediction], y=batch_a_final, verbose=0)\n",
    "        self.critic_network.fit(x=batch_s, y=batch_gae_r, epochs=1, verbose=0)\n",
    "        #soft update the target network(aka actor_old). \n",
    "        self.update_tartget_network()\n",
    "\n",
    "\n",
    "    def store_transition(self, s, a, s_, r, done):\n",
    "\n",
    "        self.memory.store(s, a, s_, r, done)\n",
    "\n",
    "\n",
    "    def get_v(self,state):\n",
    "\n",
    "        s = np.reshape(state,(-1, self.state_dim[0]))\n",
    "        v = self.critic_network.predict_on_batch(s)\n",
    "        return v\n",
    "    \n",
    "\n",
    "    def get_old_prediction(self, state):\n",
    "\n",
    "        state = np.reshape(state, (-1, self.state_dim[0]))\n",
    "        return self.actor_old_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "state_input (InputLayer)     [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 32)                256       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "actor_output_layer (Dense)   (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,345\n",
      "Trainable params: 1,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           256         state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           1056        dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "advantage_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "old_prediction_input (InputLaye [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actor_output_layer (Dense)      (None, 6)            198         dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,510\n",
      "Trainable params: 1,510\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_input (InputLayer)        [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           256         state_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           1056        dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "advantage_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "old_prediction_input (InputLaye [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actor_output_layer (Dense)      (None, 6)            198         dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,510\n",
      "Trainable params: 1,510\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-788a294c4b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# to keep the training data independant and identically distributed IID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAJECTORY_BUFFER_SIZE\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0msamples_filled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-0eddd0b8ace7>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m#important: make gae type rewards BEFORE getting random batches if not done yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAE_CALCULATED_Q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_gae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;31m#get randomized mini batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgae_r\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINING_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-0eddd0b8ace7>\u001b[0m in \u001b[0;36mmake_gae\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mgae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "TRAIN_ITERATIONS = 1000\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "TRAJECTORY_BUFFER_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "RENDER_EVERY = 100\n",
    "\n",
    "pseq=[[1,2,4,6,3,5],[3,2,6,1,4,5]]\n",
    "w=[1,2,3,2,1,1]\n",
    "h=[2,1,4,3,3,2]\n",
    "\n",
    "state = pseq[0]\n",
    "state.append(1)\n",
    "\n",
    "env = PsecWorld(state, w, h)\n",
    "agent = Agent(env.action_space.n,env.observation_space.shape,BATCH_SIZE)\n",
    "samples_filled = 0\n",
    "\n",
    "for cnt_episode in range(TRAIN_ITERATIONS):\n",
    "    s = env.reset(state)\n",
    "    r_sum = 0\n",
    "    for cnt_step in range(MAX_EPISODE_LENGTH):\n",
    "        #sometimes render\n",
    "        if cnt_episode % RENDER_EVERY == 0 :\n",
    "            env.render()\n",
    "        #get action from agent given state\n",
    "        a = agent.choose_action(s)\n",
    "        #get s_,r,done\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        r /= 100\n",
    "        r_sum += r\n",
    "        if done:\n",
    "            r = -1\n",
    "        #store transitions to agent.memory\n",
    "        agent.store_transition(s, a, s_, r, done)\n",
    "        samples_filled += 1\n",
    "        #train in batches one buffer is filled with samples.\n",
    "        if samples_filled % TRAJECTORY_BUFFER_SIZE == 0 and samples_filled != 0:\n",
    "            #To be sample efficient, sample as often as statistically necearry to \n",
    "            # use all availible samples in memory. Imortant to sample randomly \n",
    "            # to keep the training data independant and identically distributed IID\n",
    "            for _ in range(TRAJECTORY_BUFFER_SIZE // BATCH_SIZE):\n",
    "                agent.train_network()\n",
    "            agent.memory.clear()\n",
    "            samples_filled = 0\n",
    "        #set state to next_state\n",
    "        s = s_\n",
    "        if done:\n",
    "            break\n",
    "    if cnt_episode % 10 == 0:\n",
    "        print(f\"Episode:{cnt_episode}, step:{cnt_step}, r_sum:{r_sum}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
